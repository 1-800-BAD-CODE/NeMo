
init_from_nemo_model: pcs_base_bs2x16x256.nemo
name: "base_bs1x16x256"

model:
  tokenizer:
    tokenizer_name: sentencepiece
    vocab_file: null
    tokenizer_model: /home/shane/data/pcs/tokenizers/enesruzh_char2/spe_char.model
    special_tokens:
      unk_token: "<unk>"
      pad_token: "<pad>"
      bos_token: "<s>"
      eos_token: "</s>"

  language_model:
    config_file: null
    pretrained_model_name: bert-base-multilingual-cased  # Will be ignored, but cannot be null.
    nemo_file: null
    lm_checkpoint: null
    vocab_file: null
    config:
      model_type: BertModel
      vocab_size: 5212
      hidden_size: 768
      num_hidden_layers: 12
      num_attention_heads: 12
      intermediate_size: 3072
      hidden_act: gelu
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 512
      type_vocab_size: 2
      pad_token_id: 0
      position_embedding_type: absolute

  # Optionally we can learn the exclamation marks ["!", "！", "¡"]. If not using them, specify them as 'unused' so that
  # the datasets will not filter out sentences that end in '!', and therefore the model will not barf when seeing these
  # for the first time during inference.
  unused_punctuation: ["!", "！", "¡"]
  null_punct_token: "<NULL>"
  # Punctuation tokens that are predicted before each subword. Add "¡" here if using it.
  punct_pre_labels: ["<NULL>", "¿",]
  # Punctuation tokens that are predicted after each subword. Note the datasets below are configured to map the Chinese
  # enumeration comma to a regular Chinese comma, due to inconsistencies in the raw data, so it is not learned.
  punct_post_labels: [
      "<NULL>",
      ".", ",", "?",
      "？", "，", "。",
  ]
  loss:
    # Loss weights for punct_post_labels
    punct_post:
      weight: [
          1.0,
          4, 4, 4,
          4, 4, 4,
      ]
    # Loss weights for punct_pre_labels
    punct_pre:
      weight: [0.5, 4.0]
    # Loss weights for [lower_case, upper_case]
    cap:
      weight: [1.0, 8.0]
    # Loss weights for [no_stop, full_stop]
    seg:
      weight: [1.0, 2.0]
  # Target padding value. No need to change.
  pad_value: -100

  train_ds:
    batch_size: 16
    num_workers: 6
    # A list of datasets will be used to create a ConcatDataset.
    sampling_technique: "temperature"
    sampling_temperature: 5
    # Typically, set up one data set per language.
    datasets:
      # targets can be any implementation of a PunctCapSegDataset; currently, only TextPunctCapSegDataset is fully
      #  implemented.
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        # One or more unprocessed, plain-text files containing one sentence per line
        text_files:
          - "/home/shane/corpora/tatoeba/en/train.txt"
          - "/home/shane/corpora/wmt_nc/news.2021.en.shuffled.deduped"
        # Read in only the first N lines from each text file.
        max_lines_per_input_file: 300000
        # Will be used to select the punctuation targets generator, which is language-specific.
        language: "en"
        # Punctuation labels that can appear before tokens.
        punct_pre_labels: ${model.punct_pre_labels}
        # Punctuation labels that can appear after tokens.
        punct_post_labels: ${model.punct_post_labels}
        # Any unused punctuation. Typically, we filter out input lines that do not end in punctuation as a simple
        # cleaning rule. By specifying unused punctuation, e.g., '!', we can retain lines that end in these tokens and
        # let the model see them during training, even if they aren't targets, so the model handles them correctly at
        # inference time.
        unused_punctuation: ${model.unused_punctuation}
        # Filter out input lines longer than this many words, as a cleaning method (a perfect data set has only one
        # sentence per line; we use heuristics to achieve this from arbitrary data).
        max_input_length_words: 32
        min_input_length_words: 1
        # Generate examples by concatenating multiple lines together, determined by the following min/max counts.
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        # Limit examples to this many tokens. This is enforced by a simple truncation the examples after generation.
        max_length: 256
        # Truncate up to this many tokens from the end of examples, so the model doesn't learn to always put punctuation
        # at the end of each example. With moderate max_length this probably isn't needed.
        truncate_max_tokens: 0
        # List of zero or more implementations of nlp.data.token_classification.punct_cap_seg_dataset.TextCleaner
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/ru/train.txt"
          - "/home/shane/corpora/wmt_nc/news.2021.ru.shuffled.deduped"
        max_lines_per_input_file: 300000
        language: "ru"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_words: 32
        min_input_length_words: 1
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/es/train.txt"
          - "/home/shane/corpora/wmt_nc/news.2021.es.shuffled.deduped"
        max_lines_per_input_file: 300000
        language: "es"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_words: 32
        min_input_length_words: 1
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.SpanishPunctNormalizer
            pre_punct_tokens: ${model.punct_pre_labels}
            post_punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/zh/train.txt"
          - "/home/shane/corpora/wmt_nc/news.2021.zh.shuffled.deduped"
        max_lines_per_input_file: 300000
        language: "zh"
        # Let dataset know this is a continuous-script language
        is_continuous: true
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_chars: 64
        min_input_length_chars: 2
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.ChineseTextCleaner
            remove_spaces: true
            replace_latin: true
            no_enum_comma: true
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}

  validation_ds:
    batch_size: 32
    num_workers: 6
    datasets:
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
#          - "/home/shane/corpora/tatoeba/en/dev.txt"
          - "/home/shane/corpora/wmt_nc/news.2021.en.shuffled.deduped.tail5k"
        language: "en"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_words: 32
        min_input_length_words: 1
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        rng_seed: 12345
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/ru/dev.txt"
        language: "ru"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_words: 32
        min_input_length_words: 1
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        rng_seed: 12345
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/es/dev.txt"
        language: "es"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_words: 32
        min_input_length_words: 1
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        rng_seed: 12345
        cleaners:
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.SpanishPunctNormalizer
            pre_punct_tokens: ${model.punct_pre_labels}
            post_punct_tokens: ${model.punct_post_labels}
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - "/home/shane/corpora/tatoeba/zh/dev.txt"
        language: "zh"
        punct_pre_labels: ${model.punct_pre_labels}
        punct_post_labels: ${model.punct_post_labels}
        unused_punctuation: ${model.unused_punctuation}
        max_input_length_chars: 64
        min_input_length_chars: 2
        min_lines_per_eg: 1
        max_lines_per_eg: 4
        max_length: 256
        truncate_max_tokens: 0
        rng_seed: 12345
        cleaners:
          # Use a Chinese text cleaner to remove any spaces from inputs
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.ChineseTextCleaner
            remove_spaces: true
            replace_latin: true
            no_enum_comma: true
          - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.StandardPunctNormalizer
            punct_tokens: ${model.punct_post_labels}

  punct_head_pre:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  punct_head_post:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  cap_head:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  seg_head:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  optim:
    name: adam
    lr: 2e-5  # 1e-4  # 2e-5
#    betas: [0.9, 0.98]
    weight_decay: 0  # 5e-4
    sched:
      name: WarmupAnnealing
      min_lr: 1e-6
      last_epoch: -1
      warmup_ratio: null
      warmup_steps: 6000

trainer:
  devices: -1
  num_nodes: 1
  max_epochs: 1
  max_steps: 48000
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: auto
  log_every_n_steps: 250  # Interval of logging.
  val_check_interval: 4000  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: null
  logger: false
  enable_checkpointing: false
  progress_bar_refresh_rate: 10
  benchmark: false

exp_manager:
  exp_dir: nemo_exp/char/
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  # Available metrics are val_{language}_{punct_pre,punct_post,cap,seg}_{f1,precision,recall}
  # TODO use average metrics all languages
  checkpoint_callback_params:
    monitor: "val_en_punct_post_f1"
    mode: "max"
    save_top_k: 2
    always_save_nemo: true
  resume_if_exists: false
  resume_ignore_no_checkpoint: false
