
name: "punct_cap_seg"
init_from_nemo_model: null

model:
  # For reference only
  supported_languages: [
      "en", "es", "zh"
  ]
  multipass: false  # train with two passes (true) or one (false)
  pretokenize: true  # For SPE, probably true. This will pass the inputs through a cycle of tokenization/detokenization
  freeze_encoder_n_batches: null  # To freeze the encoder for the first N batches, set to a positive number.

  tokenizer:
    special_tokens: null
    tokenizer_name: ${model.language_model.pretrained_model_name}
    tokenizer_model: null
    vocab_file: null

  language_model:
    config_file: null
    pretrained_model_name: xlm-roberta-base
    nemo_file: null
    lm_checkpoint: null
    vocab_file: null
    config: null

  null_punct_token: "<NULL>"
  # Punctuation tokens that are predicted before each subword. Add "¡" here if using it.
  punct_pre_labels: ["<NULL>", "¿",]
  # Punctuation tokens that are predicted after each subword. Note the datasets below are configured to map the Chinese
  # enumeration comma to a regular Chinese comma, due to inconsistencies in the raw data, so it is not learned.
  punct_post_labels: [
      "<NULL>",
      ".", ",", "?",
      "？", "，", "。", # Chinese, no enum comma
      "、", "・",  # Japanese comma, middle dot
      "।", # Hindi
      "؟", "،", # Arabic
  ]
  # Punctuation tokens that might be full stops. Used for masking sentence boundary predictions.
  full_stops: [".", "?", "？", "。", "।", "؟"]
  loss:
    # Loss weights for [pre_punct, post_punct, cap, seg]
    agg_loss_weights: [0.25, 0.25, 0.25, 0.25]
    # Loss weights for punct_post_labels
    punct_post:
      # Weight for each positive example. Computed via `counts.pow(1/T).mean() / counts.pow(1/T)`. T is 5 here.
      weight: [0.34, 0.74, 0.8, 1.03, 1.28, 1.64, 1.36, 1.81, 2.3, 1.37, 1.49, 1.2]
    # Loss weights for punct_pre_labels
    punct_pre:
      weight: [0.56, 2.0]
    # Loss weights for [lower_case, upper_case]
    cap:
      weight: 1.0  # BCE - only positive class weight
    # Loss weights for [no_stop, full_stop]
    seg:
      weight: [1.0, 1.0]
  # Target padding value. No need to change.
  pad_value: -100

  train_ds:
    batch_size: 64
    num_workers: 4
    # A list of datasets will be used to create a ConcatMapDataset.
    sampling_technique: "temperature"
    sampling_temperature: 5
    # "common" key/value pairs will be added to every data set, unless that dataset already specifies the key.
    common:
      punct_pre_labels: ${model.punct_pre_labels}
      punct_post_labels: ${model.punct_post_labels}
      min_lines_per_eg: 2
      max_lines_per_eg: 6
      max_length: 128
      truncate_max_tokens: 0
      prob_drop_punct: 1.0
      prob_lower_case: 1.0
      multipass: ${model.multipass}
      full_stops: ${model.full_stops}
      pretokenize: ${model.pretokenize}
    # Typically, set up one data set per language.
    datasets:
      # targets can be any implementation of a PunctCapSegDataset
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        # One or more text files containing one sentence per line. Should be pre-processed by `prep_pcs_data.py` to
        # avoid errors such as consecutive punctuation marks, as well as remove bad lines.
        text_files:
          - ???
        # Will be used to select the punctuation targets generator, which is language-specific.
        language: "en"
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - ???
        language: "es"
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - ???
        language: "zh"
        # Let dataset know this is a continuous-script language
        is_continuous: true

  validation_ds:
    batch_size: 128
    num_workers: 4
    # "common" options are analogous to train_ds
    common:
      punct_pre_labels: ${model.punct_pre_labels}
      punct_post_labels: ${model.punct_post_labels}
      min_lines_per_eg: 2
      max_lines_per_eg: 6
      max_length: 128
      truncate_max_tokens: 0
      rng_seed: 12345
      prob_drop_punct: 1.0
      prob_lower_case: 1.0
      multipass: ${model.multipass}
      full_stops: ${model.full_stops}
      pretokenize: ${model.pretokenize}
    datasets:
      # Probably one for each training language, with similar parameters
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - ???
        language: "en"
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - ???
        language: "es"
      - _target_: nemo.collections.nlp.data.token_classification.punct_cap_seg_dataset.TextPunctCapSegDataset
        text_files:
          - ???
        language: "zh"
        is_continuous: true

  punct_head_pre:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  punct_head_post:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  cap_head:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  seg_head:
    num_layers: 1
    activation: relu
    dropout: 0.1
    use_transformer_init: true

  optim:
    name: adam
    lr: 2e-5
    betas: [0.9, 0.999]
    weight_decay: 0
    sched:
      name: WarmupAnnealing
      min_lr: 1e-6
      last_epoch: -1
      warmup_ratio: null
      warmup_steps: 8000

trainer:
  devices: -1
  num_nodes: 1
  max_epochs: 1
  max_steps: 50000
  accumulate_grad_batches: 1
  gradient_clip_val: 0.0
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: auto
  log_every_n_steps: 250  # Interval of logging.
  val_check_interval: 5000  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: null
  logger: false
  enable_checkpointing: false
  benchmark: false

exp_manager:
  exp_dir: nemo_exp/
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  # Available metrics are val_{language}_{punct_pre,punct_post,cap,seg}_{f1,precision,recall}
  checkpoint_callback_params:
    monitor: "val_en_punct_post_f1"
    mode: "max"
    save_top_k: 2
    always_save_nemo: true
#  # for continuing from a '*-last.ckpt' in an experiment directory
#  version: 2022-10-03_16-56-33
#  resume_if_exists: true
#  resume_ignore_no_checkpoint: false
